{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Generator for ME/CFS\n",
        "## Optimized & Modular Version\n",
        "\n",
        "This notebook generates synthetic patient data based on SF-36 inputs using a cascade of deep learning models.\n",
        "The methodology follows the graph theory analysis described in the paper *A synthetic data generation system for myalgic encephalomyelitis/chronic fatigue syndrome questionnaires*.\n",
        "\n",
        "**Requirements:**\n",
        "* TensorFlow\n",
        "* Pandas\n",
        "* Pre-trained `.h5` model files in the `./models` directory."
      ],
      "metadata": {
        "id": "GNvPM58zjoqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s8iVZ2sjf6u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# GPU Configuration\n",
        "# Checks if a GPU is available and sets memory growth to avoid allocating all memory at once.\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(physical_devices) > 0}\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. Column Definitions\n",
        "# ==========================================\n",
        "# These lists define the specific column names for each questionnaire,\n",
        "# matching the output layers of the pre-trained models.\n",
        "\n",
        "COLS_HAD = [f'had_{i}' for i in range(1, 15)]\n",
        "\n",
        "COLS_SCL90R = [f'SCL{i}' for i in range(1, 91)]\n",
        "\n",
        "COLS_FIS8 = [f'fis{i}' for i in range(1, 9)]\n",
        "\n",
        "# FIS40 uses a mixed naming convention in the original dataset:\n",
        "# The first 8 columns have a '_y' suffix, followed by standard numbering.\n",
        "COLS_FIS40 = [f'fis{i}_y' for i in range(1, 9)] + [f'fis{i}' for i in range(9, 41)]\n",
        "\n",
        "COLS_PSQI = [f'comp{i}' for i in range(1, 8)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Core Functions\n",
        "# ==========================================\n",
        "\n",
        "def predict_questionnaire(input_data, col_names, model_filename_template, models_dir='.'):\n",
        "    \"\"\"\n",
        "    Generic function to predict a full questionnaire column by column.\n",
        "\n",
        "    Args:\n",
        "        input_data (pd.DataFrame): The accumulated input matrix (previous steps).\n",
        "        col_names (list): List of target column names to predict.\n",
        "        model_filename_template (str): Filename template (e.g., 'HADW_model_{col}.h5').\n",
        "        models_dir (str): Directory path containing the .h5 files.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the predictions for this specific questionnaire.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "\n",
        "    # print(f\"Processing {len(col_names)} items for pattern: {model_filename_template}...\")\n",
        "\n",
        "    for col in col_names:\n",
        "        model_path = os.path.join(models_dir, model_filename_template.format(col=col))\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "\n",
        "        # Loading with compile=False is significantly faster for inference\n",
        "        # as it skips loading optimizers and loss functions.\n",
        "        model = load_model(model_path, compile=False)\n",
        "\n",
        "        # Perform prediction\n",
        "        pred_probs = model.predict(input_data, verbose=0)\n",
        "\n",
        "        # Get the class with the highest probability (argmax)\n",
        "        # Assumes the output is categorical classification (0, 1, 2, 3...)\n",
        "        predictions[col] = pred_probs.argmax(axis=1)\n",
        "\n",
        "    return pd.DataFrame(predictions)\n",
        "\n",
        "def generate_synthetic_data(sf36_input, models_dir='.'):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for all questionnaires using the cascade method.\n",
        "\n",
        "    Architecture defined in the paper:\n",
        "    Flow: SF36 -> HAD -> SCL90R -> FIS8 -> FIS40 -> PSQI\n",
        "\n",
        "    Args:\n",
        "        sf36_input (pd.DataFrame): Validated SF-36 responses (n_samples, 36).\n",
        "        models_dir (str): Path to the folder containing .h5 model weights.\n",
        "\n",
        "    Returns:\n",
        "        tuple: DataFrames for (HAD, SCL90R, FIS8, FIS40, PSQI)\n",
        "    \"\"\"\n",
        "\n",
        "    # Input validation\n",
        "    if sf36_input.shape[1] != 36:\n",
        "        print(f\"Warning: Expected 36 input columns, received {sf36_input.shape[1]}.\")\n",
        "\n",
        "    # Step 1: Predict HAD (Input: SF36)\n",
        "    print(\"--- Step 1: Generating HAD ---\")\n",
        "    df_had = predict_questionnaire(sf36_input, COLS_HAD, 'HADW_model_{col}.h5', models_dir)\n",
        "\n",
        "    # Concatenate input + prediction for the next step\n",
        "    X_step1 = pd.concat([sf36_input.reset_index(drop=True), df_had], axis=1)\n",
        "\n",
        "    # Step 2: Predict SCL90R (Input: SF36 + HAD)\n",
        "    print(\"--- Step 2: Generating SCL-90-R ---\")\n",
        "    df_scl = predict_questionnaire(X_step1, COLS_SCL90R, 'SCLW_model_{col}.h5', models_dir)\n",
        "    X_step2 = pd.concat([X_step1, df_scl], axis=1)\n",
        "\n",
        "    # Step 3: Predict FIS8 (Input: SF36 + HAD + SCL90R)\n",
        "    print(\"--- Step 3: Generating FIS-8 ---\")\n",
        "    df_fis8 = predict_questionnaire(X_step2, COLS_FIS8, 'fis8_model_{col}.h5', models_dir)\n",
        "    X_step3 = pd.concat([X_step2, df_fis8], axis=1)\n",
        "\n",
        "    # Step 4: Predict FIS40 (Input: ... + FIS8)\n",
        "    print(\"--- Step 4: Generating FIS-40 ---\")\n",
        "    df_fis40 = predict_questionnaire(X_step3, COLS_FIS40, 'fis40_model_{col}.h5', models_dir)\n",
        "    X_step4 = pd.concat([X_step3, df_fis40], axis=1)\n",
        "\n",
        "    # Step 5: Predict PSQI (Input: ... + FIS40)\n",
        "    print(\"--- Step 5: Generating PSQI ---\")\n",
        "    df_psqi = predict_questionnaire(X_step4, COLS_PSQI, 'psqi_model_{col}.h5', models_dir)\n",
        "\n",
        "    return df_had, df_scl, df_fis8, df_fis40, df_psqi"
      ],
      "metadata": {
        "id": "ZwZ2zjJajuUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Execution Block\n",
        "# ==========================================\n",
        "\n",
        "# Settings\n",
        "# Update these paths if running in Google Colab with Drive mounted\n",
        "# e.g., models_dir = '/content/drive/MyDrive/SFCSyntheticDataGenerator/models'\n",
        "MODELS_DIR = './models'\n",
        "INPUT_FILE = 'sf36.csv'\n",
        "\n",
        "if os.path.exists(INPUT_FILE):\n",
        "    print(\"Loading input data...\")\n",
        "    sf36_df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "    # Basic Data Cleaning\n",
        "    if 'Unnamed: 0' in sf36_df.columns:\n",
        "        sf36_df = sf36_df.drop(['Unnamed: 0'], axis=1)\n",
        "    sf36_df = sf36_df.dropna()\n",
        "\n",
        "    # Ensure we are using the correct columns (assuming first 36 are SF-36)\n",
        "    # Adjust .iloc if your csv structure is different\n",
        "    X_input = sf36_df.iloc[:, :36]\n",
        "\n",
        "    try:\n",
        "        # Run the generator\n",
        "        print(\"Starting generation process...\")\n",
        "        had, scl90r, fis8, fis40, psqi = generate_synthetic_data(X_input, models_dir=MODELS_DIR)\n",
        "\n",
        "        print(\"\\nGeneration completed successfully!\")\n",
        "        print(f\"Output Shapes -> HAD: {had.shape}, SCL: {scl90r.shape}, PSQI: {psqi.shape}\")\n",
        "\n",
        "        # Save results example\n",
        "        # had.to_csv('synthetic_HAD.csv', index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during execution: {e}\")\n",
        "        print(\"Please ensure you have downloaded the weights and the path is correct.\")\n",
        "else:\n",
        "    print(f\"Input file '{INPUT_FILE}' not found. Please upload it to the runtime.\")"
      ],
      "metadata": {
        "id": "hi3N_q4ljwir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}